import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._
import java.util.regex.Pattern;
import java.util.regex.Matcher;
import org.apache.spark.sql.functions.udf
import scala.collection.mutable.ListBuffer

spark = SparkSession.builder().appName("IsbnEncoderTest").master("local").getOrCreate()

val is_ISBN : (String) => Array[String] =(column: String) => {

      val regex = "^(?:ISBN(?:-1[03])?:? )?(?=[0-9X]{10}$|(?=(?:[0-9]+[- ]){3})[- 0-9X]{13}$|97[89][0-9]{10}$|(?=(?:[0-9]+[- ]){4})[- 0-9]{17}$)(?:97[89][- ]?)?[0-9]{1,5}[- ]?[0-9]+[- ]?[0-9]+[- ]?[0-9X]$";
 
	 val pattern = Pattern.compile(regex)
	 var dg =  column.replaceAll("[^0-9]", "")
	 var matcher = pattern.matcher(dg)
	 var allISBNs = new ListBuffer[String]()
	 allISBNs += column
	 if(matcher.matches())
	{
	 allISBNs += "ISBN-EAN: "+dg.substring(0,3)
	 allISBNs += "ISBN-GROUP: "+dg.substring(3,5)
   allISBNs += "ISBN-PUBLISHER: "+dg.substring(5,9)
   allISBNs += "ISBN-TITLE: "+dg.substring(9,12)
	}

	allISBNs.toArray
   }
   
   val addColumnUDF = udf(is_ISBN)
   val newDF = df.withColumn("newC",addColumnUDF(df.col("isbn")))
   
   val selectedColumns = newDF.select("name","year","newC")
  
  selectedColumns.withColumn("isbn", explode($"newC")).show()
